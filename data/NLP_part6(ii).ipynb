{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\andrehadianto\\\\Documents\\\\CodingProjects\\\\50.040-NLP-Sentiment-Analysis\\\\data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/18/2020 18:01:57 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "08/18/2020 18:01:58 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at C:\\Users\\andrehadianto\\.cache\\torch\\pytorch_transformers\\5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "08/18/2020 18:01:59 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at C:\\Users\\andrehadianto\\.cache\\torch\\pytorch_transformers\\b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "08/18/2020 18:01:59 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 19,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "08/18/2020 18:02:00 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at C:\\Users\\andrehadianto\\.cache\\torch\\pytorch_transformers\\35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "08/18/2020 18:02:02 - INFO - pytorch_transformers.modeling_utils -   Weights of Ner not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "08/18/2020 18:02:02 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in Ner: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   *** Example ***\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   guid: train-0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   tokens: The official cause of death has not been officially determined , but investigators believe the 36 - year - old writer died from a self - inflicted gunshot wound .\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_ids: 101 1109 2078 2612 1104 1473 1144 1136 1151 3184 3552 117 1133 17718 2059 1103 3164 118 1214 118 1385 2432 1452 1121 170 2191 118 18567 24939 5785 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   *** Example ***\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   guid: train-1\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   tokens: Mr . O ##mi called for stronger political will by Asian governments to stop the spread of the disease .\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_ids: 101 1828 119 152 3080 1270 1111 5992 1741 1209 1118 3141 6670 1106 1831 1103 2819 1104 1103 3653 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   *** Example ***\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   guid: train-2\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   tokens: This is the second U . N . - Congo ##les ##e offensive against militia ##s in the region since the DR ##C ' s constitutional referendum a week ago .\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_ids: 101 1188 1110 1103 1248 158 119 151 119 118 8695 2897 1162 5810 1222 10193 1116 1107 1103 1805 1290 1103 22219 1658 112 188 7950 9905 170 1989 2403 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   *** Example ***\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   guid: train-3\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   tokens: Wednesday , officials at the meeting , including Sudan ' s President Omar Hassan al - Ba ##shi ##r , asked media ##tors to prepare for a new round of talks later this month .\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_ids: 101 9031 117 3878 1120 1103 2309 117 1259 10299 112 188 1697 13569 13583 2393 118 18757 5933 1197 117 1455 2394 5067 1106 7034 1111 170 1207 1668 1104 7430 1224 1142 2370 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   *** Example ***\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   guid: train-4\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   tokens: Burmese state media reported Wednesday that border police seized a large quantity of heroin and other illegal drugs near the border with Thailand this week .\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_ids: 101 15080 1352 2394 2103 9031 1115 3070 2021 7842 170 1415 11978 1104 22856 1105 1168 5696 5557 1485 1103 3070 1114 5872 1142 1989 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/18/2020 18:02:03 - INFO - __main__ -   ***** Running training *****\n",
      "08/18/2020 18:02:03 - INFO - __main__ -     Num examples = 700\n",
      "08/18/2020 18:02:03 - INFO - __main__ -     Batch size = 32\n",
      "08/18/2020 18:02:03 - INFO - __main__ -     Num steps = 63\n",
      "\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[AC:\\Users\\andrehadianto\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_transformers\\optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "\n",
      "Iteration:   5%|4         | 1/22 [01:02<21:52, 62.49s/it]\u001b[A\n",
      "\n",
      "Iteration:   9%|9         | 2/22 [02:07<21:04, 63.22s/it]\u001b[A\n",
      "\n",
      "Iteration:  14%|#3        | 3/22 [03:13<20:15, 63.96s/it]\u001b[A\n",
      "\n",
      "Iteration:  18%|#8        | 4/22 [04:18<19:17, 64.30s/it]\u001b[A\n",
      "\n",
      "Iteration:  23%|##2       | 5/22 [05:23<18:18, 64.63s/it]\u001b[A\n",
      "\n",
      "Iteration:  27%|##7       | 6/22 [06:29<17:18, 64.91s/it]\u001b[A\n",
      "\n",
      "Iteration:  32%|###1      | 7/22 [07:28<15:48, 63.26s/it]\u001b[A\n",
      "\n",
      "Iteration:  36%|###6      | 8/22 [08:33<14:53, 63.81s/it]\u001b[A\n",
      "\n",
      "Iteration:  41%|####      | 9/22 [09:38<13:53, 64.10s/it]\u001b[A\n",
      "\n",
      "Iteration:  45%|####5     | 10/22 [10:43<12:52, 64.37s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 11/22 [11:49<11:53, 64.90s/it]\u001b[A\n",
      "\n",
      "Iteration:  55%|#####4    | 12/22 [12:56<10:55, 65.53s/it]\u001b[A\n",
      "\n",
      "Iteration:  59%|#####9    | 13/22 [14:03<09:53, 65.89s/it]\u001b[A\n",
      "\n",
      "Iteration:  64%|######3   | 14/22 [15:10<08:49, 66.16s/it]\u001b[A\n",
      "\n",
      "Iteration:  68%|######8   | 15/22 [16:16<07:42, 66.14s/it]\u001b[A\n",
      "\n",
      "Iteration:  73%|#######2  | 16/22 [17:22<06:37, 66.27s/it]\u001b[A\n",
      "\n",
      "Iteration:  77%|#######7  | 17/22 [18:28<05:31, 66.23s/it]\u001b[A\n",
      "\n",
      "Iteration:  82%|########1 | 18/22 [19:35<04:25, 66.29s/it]\u001b[A\n",
      "\n",
      "Iteration:  86%|########6 | 19/22 [20:41<03:19, 66.37s/it]\u001b[A\n",
      "\n",
      "Iteration:  91%|######### | 20/22 [21:48<02:13, 66.52s/it]\u001b[A\n",
      "\n",
      "Iteration:  95%|#########5| 21/22 [22:55<01:06, 66.51s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 22/22 [23:48<00:00, 62.57s/it]\u001b[A\n",
      "Iteration: 100%|##########| 22/22 [23:48<00:00, 64.94s/it]\n",
      "\n",
      "Epoch:  33%|###3      | 1/3 [23:48<47:37, 1428.63s/it]\n",
      "\n",
      "Iteration:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:   5%|4         | 1/22 [01:06<23:07, 66.07s/it]\u001b[A\n",
      "\n",
      "Iteration:   9%|9         | 2/22 [02:12<22:02, 66.12s/it]\u001b[A\n",
      "\n",
      "Iteration:  14%|#3        | 3/22 [03:18<20:57, 66.16s/it]\u001b[A\n",
      "\n",
      "Iteration:  18%|#8        | 4/22 [04:24<19:51, 66.18s/it]\u001b[A\n",
      "\n",
      "Iteration:  23%|##2       | 5/22 [05:31<18:46, 66.29s/it]\u001b[A\n",
      "\n",
      "Iteration:  27%|##7       | 6/22 [06:37<17:41, 66.34s/it]\u001b[A\n",
      "\n",
      "Iteration:  32%|###1      | 7/22 [07:44<16:35, 66.37s/it]\u001b[A\n",
      "\n",
      "Iteration:  36%|###6      | 8/22 [08:49<15:26, 66.15s/it]\u001b[A\n",
      "\n",
      "Iteration:  41%|####      | 9/22 [09:56<14:22, 66.34s/it]\u001b[A\n",
      "\n",
      "Iteration:  45%|####5     | 10/22 [11:02<13:14, 66.18s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 11/22 [12:08<12:08, 66.22s/it]\u001b[A\n",
      "\n",
      "Iteration:  55%|#####4    | 12/22 [13:15<11:02, 66.26s/it]\u001b[A\n",
      "\n",
      "Iteration:  59%|#####9    | 13/22 [14:21<09:55, 66.21s/it]\u001b[A\n",
      "\n",
      "Iteration:  64%|######3   | 14/22 [15:26<08:48, 66.07s/it]\u001b[A\n",
      "\n",
      "Iteration:  68%|######8   | 15/22 [16:33<07:43, 66.17s/it]\u001b[A\n",
      "\n",
      "Iteration:  73%|#######2  | 16/22 [17:39<06:36, 66.15s/it]\u001b[A\n",
      "\n",
      "Iteration:  77%|#######7  | 17/22 [18:45<05:30, 66.05s/it]\u001b[A\n",
      "\n",
      "Iteration:  82%|########1 | 18/22 [19:51<04:24, 66.12s/it]\u001b[A\n",
      "\n",
      "Iteration:  86%|########6 | 19/22 [20:58<03:18, 66.23s/it]\u001b[A\n",
      "\n",
      "Iteration:  91%|######### | 20/22 [22:04<02:12, 66.25s/it]\u001b[A\n",
      "\n",
      "Iteration:  95%|#########5| 21/22 [23:10<01:06, 66.15s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 22/22 [24:03<00:00, 62.35s/it]\u001b[A\n",
      "Iteration: 100%|##########| 22/22 [24:03<00:00, 65.63s/it]\n",
      "\n",
      "Epoch:  67%|######6   | 2/3 [47:52<23:53, 1433.17s/it]\n",
      "\n",
      "Iteration:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:   5%|4         | 1/22 [01:05<22:48, 65.16s/it]\u001b[A\n",
      "\n",
      "Iteration:   9%|9         | 2/22 [02:11<21:48, 65.43s/it]\u001b[A\n",
      "\n",
      "Iteration:  14%|#3        | 3/22 [03:16<20:44, 65.50s/it]\u001b[A\n",
      "\n",
      "Iteration:  18%|#8        | 4/22 [04:22<19:40, 65.60s/it]\u001b[A\n",
      "\n",
      "Iteration:  23%|##2       | 5/22 [05:29<18:38, 65.81s/it]\u001b[A\n",
      "\n",
      "Iteration:  27%|##7       | 6/22 [06:34<17:32, 65.76s/it]\u001b[A\n",
      "\n",
      "Iteration:  32%|###1      | 7/22 [07:40<16:27, 65.85s/it]\u001b[A\n",
      "\n",
      "Iteration:  36%|###6      | 8/22 [08:46<15:22, 65.88s/it]\u001b[A\n",
      "\n",
      "Iteration:  41%|####      | 9/22 [09:52<14:17, 65.94s/it]\u001b[A\n",
      "\n",
      "Iteration:  45%|####5     | 10/22 [10:59<13:12, 66.04s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 11/22 [12:02<11:59, 65.39s/it]\u001b[A\n",
      "\n",
      "Iteration:  55%|#####4    | 12/22 [13:09<10:56, 65.69s/it]\u001b[A\n",
      "\n",
      "Iteration:  59%|#####9    | 13/22 [14:15<09:51, 65.77s/it]\u001b[A\n",
      "\n",
      "Iteration:  64%|######3   | 14/22 [15:21<08:48, 66.01s/it]\u001b[A\n",
      "\n",
      "Iteration:  68%|######8   | 15/22 [16:28<07:42, 66.07s/it]\u001b[A\n",
      "\n",
      "Iteration:  73%|#######2  | 16/22 [17:34<06:36, 66.04s/it]\u001b[A\n",
      "\n",
      "Iteration:  77%|#######7  | 17/22 [18:40<05:30, 66.09s/it]\u001b[A\n",
      "\n",
      "Iteration:  82%|########1 | 18/22 [19:46<04:24, 66.03s/it]\u001b[A\n",
      "\n",
      "Iteration:  86%|########6 | 19/22 [20:51<03:17, 65.94s/it]\u001b[A\n",
      "\n",
      "Iteration:  91%|######### | 20/22 [21:57<02:11, 65.93s/it]\u001b[A\n",
      "\n",
      "Iteration:  95%|#########5| 21/22 [23:03<01:05, 65.87s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 22/22 [23:57<00:00, 62.29s/it]\u001b[A\n",
      "Iteration: 100%|##########| 22/22 [23:57<00:00, 65.34s/it]\n",
      "\n",
      "Epoch: 100%|##########| 3/3 [1:11:49<00:00, 1434.44s/it]\n",
      "Epoch: 100%|##########| 3/3 [1:11:49<00:00, 1436.60s/it]\n"
     ]
    }
   ],
   "source": [
    "!python bert-ner_run.py --no_cuda --data_dir=partial/ --bert_model=bert-base-cased --task_name=ner --output_dir=out_ner --max_seq_length=128 --do_train --num_train_epochs 3 --warmup_proportion=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andrehadianto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to predict Entity: British forces, based in the mainly south, have suffered far fewer losses than the much larger U.S. force fighting Sunni Arab insurgents and foreign fighters in the rest of Iraq.\n",
      "{'word': 'British', 'tag': 'B-gpe', 'confidence': 0.5998727679252625}\n",
      "{'word': 'forces', 'tag': 'O', 'confidence': 0.9960131645202637}\n",
      "{'word': ',', 'tag': 'O', 'confidence': 0.9965710639953613}\n",
      "{'word': 'based', 'tag': 'O', 'confidence': 0.996905505657196}\n",
      "{'word': 'in', 'tag': 'O', 'confidence': 0.9965936541557312}\n",
      "{'word': 'the', 'tag': 'O', 'confidence': 0.9960546493530273}\n",
      "{'word': 'mainly', 'tag': 'O', 'confidence': 0.9910088181495667}\n",
      "{'word': 'south', 'tag': 'O', 'confidence': 0.9807453155517578}\n",
      "{'word': ',', 'tag': 'O', 'confidence': 0.9971997737884521}\n",
      "{'word': 'have', 'tag': 'O', 'confidence': 0.9975717663764954}\n",
      "{'word': 'suffered', 'tag': 'O', 'confidence': 0.9974257349967957}\n",
      "{'word': 'far', 'tag': 'O', 'confidence': 0.9960442781448364}\n",
      "{'word': 'fewer', 'tag': 'O', 'confidence': 0.9973483085632324}\n",
      "{'word': 'losses', 'tag': 'O', 'confidence': 0.9976798892021179}\n",
      "{'word': 'than', 'tag': 'O', 'confidence': 0.9974265694618225}\n",
      "{'word': 'the', 'tag': 'O', 'confidence': 0.997221827507019}\n",
      "{'word': 'much', 'tag': 'O', 'confidence': 0.9960616230964661}\n",
      "{'word': 'larger', 'tag': 'O', 'confidence': 0.997534990310669}\n",
      "{'word': 'U.S.', 'tag': 'B-geo', 'confidence': 0.7456303238868713}\n",
      "{'word': 'force', 'tag': 'O', 'confidence': 0.9964680671691895}\n",
      "{'word': 'fighting', 'tag': 'O', 'confidence': 0.9975874423980713}\n",
      "{'word': 'Sunni', 'tag': 'O', 'confidence': 0.9605178833007812}\n",
      "{'word': 'Arab', 'tag': 'O', 'confidence': 0.5806556940078735}\n",
      "{'word': 'insurgents', 'tag': 'O', 'confidence': 0.9968796968460083}\n",
      "{'word': 'and', 'tag': 'O', 'confidence': 0.9973598122596741}\n",
      "{'word': 'foreign', 'tag': 'O', 'confidence': 0.9971575736999512}\n",
      "{'word': 'fighters', 'tag': 'O', 'confidence': 0.9976492524147034}\n",
      "{'word': 'in', 'tag': 'O', 'confidence': 0.9969857335090637}\n",
      "{'word': 'the', 'tag': 'O', 'confidence': 0.9965174198150635}\n",
      "{'word': 'rest', 'tag': 'O', 'confidence': 0.9885882139205933}\n",
      "{'word': 'of', 'tag': 'O', 'confidence': 0.9837090969085693}\n",
      "{'word': 'Iraq', 'tag': 'B-geo', 'confidence': 0.91545569896698}\n",
      "{'word': '.', 'tag': 'O', 'confidence': 0.9966469407081604}\n"
     ]
    }
   ],
   "source": [
    "from bert import Ner\n",
    "model = Ner('out_ner/')\n",
    "\n",
    "text = 'British forces, based in the mainly south, have suffered far fewer losses than the much larger U.S. force fighting Sunni Arab insurgents and foreign fighters in the rest of Iraq.'\n",
    "print(\"Text to predict Entity:\", text)\n",
    "\n",
    "output = model.predict(text)\n",
    "for prediction in output:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"partial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "with open(os.path.join(dataset, \"dev.in\")) as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = list()\n",
    "        sent = \"\"\n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()\n",
    "            if(len(formatted_line) ==0):\n",
    "                sentences.append(sent.strip())\n",
    "                sent = \"\"\n",
    "                continue\n",
    "            sent += formatted_line+\" \"    \n",
    "            \n",
    "output_filename='dev.p6.model.out'\n",
    "with open(os.path.join(dataset, output_filename), \"w\") as wf:\n",
    "    for sentence in sentences:\n",
    "        output = model.predict(sentence)\n",
    "        for prediction in output:\n",
    "            wf.write(prediction['word'] + \" \" + prediction['tag'] + \"\\n\")               \n",
    "        wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The O\n",
      "\n",
      "processed 2097 tokens with 236 phrases; found: 264 phrases; correct: 146.\n",
      "accuracy:  63.24%; (non-O)\n",
      "accuracy:  93.04%; precision:  55.30%; recall:  61.86%; FB1:  58.40\n",
      "              art: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              eve: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              geo: precision:  58.82%; recall:  70.59%; FB1:  64.17  102\n",
      "              gpe: precision:  81.82%; recall:  72.00%; FB1:  76.60  22\n",
      "              nat: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "              org: precision:  27.66%; recall:  37.14%; FB1:  31.71  47\n",
      "              per: precision:  63.33%; recall:  59.38%; FB1:  61.29  30\n",
      "              tim: precision:  57.14%; recall:  67.92%; FB1:  62.07  63\n",
      "(55.3030303030303, 61.86440677966102, 58.400000000000006)\n"
     ]
    }
   ],
   "source": [
    "from conlleval2 import evaluate, evaluate_conll_file\n",
    "\n",
    "def eval(pred,gold):\n",
    "    f_pred = open(pred,encoding = 'utf-8')\n",
    "    f_gold = open(gold,encoding = 'utf-8')\n",
    "    data_pred = f_pred.readlines()\n",
    "    data_gold = f_gold.readlines()\n",
    "    gold_tags = list()\n",
    "    pred_tags = list()\n",
    "    print(data_pred[0])\n",
    "    for sentence in range(len(data_pred)):\n",
    "        words_pred = data_pred[sentence].strip().split(' ')    \n",
    "        words_gold = data_gold[sentence].strip().split(' ')  \n",
    "        if len(words_gold)==1:\n",
    "            continue\n",
    "        gold_tags.append(words_gold[1])\n",
    "        pred_tags.append(words_pred[1])\n",
    "    return gold_tags,pred_tags\n",
    "\n",
    "g_tags, p_tags = eval(os.path.join(dataset, 'dev.p6.model.out'), os.path.join(dataset, 'dev.out'))\n",
    "print(evaluate(g_tags,p_tags,verbose=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
